[General]
verbose = True
experiment_name = nghia_CNN
results_dir = ./results/
dataset = CIFAR-10
dataset_dir = /opt/OPAL/Datasets/CIFAR10
network_type = CNN
metric = top1
callbacks = [keras.callbacks.EarlyStopping(monitor='loss', patience=2, verbose=True), keras.callbacks.ReduceLROnPlateau(monitor='loss', factor=0.2, patience=0, verbose=True, mode='auto', cooldown=0, min_lr=0)]

[DSE]
max_samples = 1e10
max_iterations = 200
initial_iterations = 10
storage_period = 10
sigma = 3
alpha = 1e-4


[Cost]
weight_cost = 1.0
mac_cost = 0.0072
max_cost = 1e4


[Parameters]
conv_depth = np.arange(3, 5, dtype='int32')
fc_depth = np.int32([1, 2])
fc_layers = np.int32(2**np.arange(start=5, stop=9))
fc_layers_local = ['fc_depth']
conv_filters = np.int32(2**np.arange(start=4, stop=7))
conv_filters_local = ['conv_depth']
conv_activation = ['ReLU']
fc_activation = ['ReLU']
out_activation = ['softmax']
conv_pooltype = ['max2d']
conv_pools = [1, 2]
conv_pools_local = ['conv_depth']
conv_kernelx = [3]
conv_kernely = [3]
conv_kernelx_local = ['conv_depth']
conv_kernely_local = ['conv_depth']
conv_strides = [1, 2, 4]
conv_strides_local = ['conv_depth']
learning_rate = np.logspace(start=np.log10(0.001), stop=np.log10(0.2), num=15, dtype='float32')
epochs = [400]
batchsize = [400]
updates = ['ADAM']
loss = ['categorical_crossentropy']


[RSM]
network_type = MLP
loss = MSE
epochs = 10 
learning_rate = 0.1
learning_alg = SGD
fc_layers = [250, 250]
fc_dropouts = None
fc_activation = ReLU
out_activation = linear


